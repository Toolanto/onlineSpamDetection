\chapter{Introduzione}
\lstset{basicstyle=\small\ttfamily,keywordstyle=\color{black}\bfseries,commentstyle=\color{darkgray},stringstyle=\color{black},showstringspaces=true}

Con questa tesi si ha come obbiettivo di studiare e analizzare le varie tecniche di spam detection ed in particolare analizzare le tecniche online. Le tecninche sono classificate sulla base dei segnali che utilizzano. Il fattore chiave è che non ci sono, o meglio sono poche, al momento tecniche online di spam detection, ovvero tecninche che rilevano lo spam durante la fase di crawling. Infatti quasi tutti i metodi tentano di fare il crawling dell'intero web e successivamente classificare le pagine in classi, di norma le classi sono due: spam oppure buone.

Il feonomeno del web spam è sempre più presente all'interno del web, questo è dovuto al fatto che gli utenti tendono ad esaminare solo i primi risultati calcolati dai motori di ricerca e quindi se un sito fa parte degli \textit{n} primi risultati, ha un ritorno economico legato alla quantità di traffico che viene generata per quel sito. Uno studio  del 2005 descritto in \cite{Nicholas:2005}, stima che la perdita finanziaria mondiale causata dallo spam e di circa 50 miliardi di dollari e nel 2009 (come descritto in \cite{Nicholas:2009}) è salita a 130 miliardi di dollari. Per questo motivo, recentemente tutte le più grandi compagnie di motori di ricerca hanno identificato il recupero di informazioni non pertinenti come una delle priorità da risolvere. Le conseguenze del web spam possono essere \cite{Spirin:2012:SWS:2207243.2207252}:
\begin{itemize}
 \item la qualità delle ricerche è compromessa penalizzando i legittimi siti web;
 \item un utente potrebbe perdere la fiducia sulla qualità di un motore di ricerca e perciò passare con facilità all'utilizzo di un altro;
 \item inoltre i siti spam possono essere usati come mezzo per malware, pubblicazione di contentuto per adulti e attacchi di tipo ``fishing''.Un prova tangibile si può vedere in \cite{Eiron:2004:RWF:988672.988714}, dove gli autori hanno eseguito l'algoritmo di \textit{PageRank} su 100 milioni di pagine e hanno notato che 11 su i primi 20 risultati erano composti da siti con contenuto per adulti.
\end{itemize}
Queste considerazioni evidenziano che quando si progetta un motore di ricerca bisogna tenere conto delle pagine che potrebbero portare al mal funzionamento del motore stesso.
%questa parte forse è da cambiare in quanto il lavoro sta cambiando infatti dallo sviluppo di un modulo siamo passati a fare dei test degli algoritmi di spam
%detection su grafo
Il lavoro prodotto sarà utilizzato per essere integrato	all'interno di un web crawler distribuito ad alte prestazioni. L'esigenza di tale modulo è sorta a seguito dello sviluppo, presso il Dipartimento, di un crawler chiamato {\itshape BubiNG}, altamente configurabile ma privo al momento di qualunque forma di rilevazione di siti e contenuti malevoli. Il problema è estremamente interessante sia dal punto di vista teorico che da quello pratico: infatti, sebbene
siano numerose le tecniche descritte in letteratura per la determinazione di spam (usando come segnali sia il contenuto che la struttura dei link), è sorprendentemente scarso l'insieme di tali tecniche che possono essere usate on-line durante il crawl. Il problema diventa ancora più complesso se si aggiungono considerazioni legate ai vincoli di spazio di memoria disponibile e tempo di calcolo.
Infatti in lettaratura il processo di spam detection viene eseguito subito dopo la fase di crawling. Ovvero il processo è composto dai seguenti passi:
\begin{itemize}
 \item crawling dell'intero web;
 \item fase di spam detection;
 \item indicizzazione.
\end{itemize}
Questo modello è utile perché molte delle tecniche utilizzate fanno delle analisi sul grafo che è il risultato della fine del processo di crawling. Da queste considerazioni noi proviamo a fare delle analisi per determinare se il processo di spam detection può essere fatto durante la fase di crawling ovvero al momento in cui il crawler esegue il ``fetch'' di una pagina per determinare ``on the fly'' se la pagina è buona o ha un contenuto malevolo. 

\section{Ranking dei motori di ricerca}
Prima di spiegare i vari metodi per fare web spam e successivamente quelli utili ad identificarlo, è necessario capire come i motori di ricerca sono capaci di valutare la rilevanza di una pagina web per una determinta query.
%preso dalle dispense

In linea di massima un sistema di reperimento di informazioni ovvero un motore di ricerca è dato da una collezione documentale \textit{D} (un insieme di documenti) di dimensione \textit{N} , da un insieme \textit{Q} di interrogazioni, e da funzione di ranking (\(r : Q \times D \mapsto R\)) che
assegna a ogni coppia formata da un’interrogazione e un documento un numero reale. L’idea è che a fronte di un’interrogazione a ogni documento viene assegnato un punteggio reale: i documenti con punteggio nullo non sono considerati rilevanti, mentre quelli a punteggio non nullo sono tanto più rilevanti quanto il punteggio è alto. In particolare i metodi di ranking si dividono in \textit{endogeni} ed \textit{esogeni}. I primi metodi fanno uso del contenuto del documento per valutarne la rilevanza mentre i secondi fanno uso di un struttura esterna che nel caso del web è il grafo composto dai collegamenti ipertestuali tra le pagine. Tra i metodi esogeni sono di maggiore importanza \textit{tf-idf} e \textit{BM25} mentre tra quelli esogeni i più diffusi in letteratura sono \textit{PageRank} e \textit{HITS}.

\subsection{Metodi di ranking endogeno}
L'algoritmo usato dai motori di ricerca per fare il rank delle pagine web basandosi sui campi di testo usa varie forme del \textit{tf-idf}. Il \textit{tf-idf} e un metodo di ranking endogeno che utilizza il contenuto di una pagina per assegnarle un punteggio. Il \textit{tf-idf} è una misura composta da due misure più semplici: la \textit{Term Frequency} e la \textit{Inverse Document Frequency}. Il primo metodo assegna a un documento \textit{d} il punteggio dato dalla somma dei conteggi dei termini \textit{t} dell'interrogazione che compaiono nel documento stesso. In questo modo documenti che hanno termini che compaiono più frequentemente avranno un punteggio più elevato. Utilizzare solo questo metodo non conviene in quanto è facilmente manipolabile. Inoltre non tiene conto del fatto che alcuni termini occorono più frequentemente non perché rilevanti, ma perché altamente frequenti all'interno di \textit{ogni} documento. Ad esempio le congiunzioni. Il secondo metodo è definito come l'inverso del numero di 
documenti nella collezione che contengono il termine \textit{t} \cite{Manning:2008:IIR:1394399p117}.
\begin{equation}
 idf_t=\log\frac{N}{df_t}
 \label{eq:idf}
\end{equation}
La combinazione del \textit{tf} ed dell' \textit{idf} produce una misura composta che permette di normalizzare il peso dei termini. Il \textit{tf-idf} di un documento \textit{d} rispetto a una query \textit{q} è calcolato su tutti i termini \textit{t} in comune come:
\begin{equation}
 tf-idf(d,q)=\sum_{t \in d \: and \: t \in q} tf(t) \cdot idf(t)
\end{equation}
Con il \textit{tf-idf} gli spammers possono avere due obbiettivi: o creare pagine rilevanti per un gran numero di query o creare pagine molto rilevanti per una specifica query. Il primo obbiettivo può essere finalizato includendo un gran numero di termini distinti in un documento. Il secondo attraverso la ripetizione di determinati termini nel documento. Ma il più delle volte i motori di ricerca non considerano l'\textit{idf} e perciò per incrementare il \textit{tf-idf} conviene incrementare la frequenza dei termini.

Anche se il \textit{tf-idf} riesce a pesare abbastanza bene i vari termini ha molti limiti e per questo che il sistema di pesatura più attualmente usato è \textit{BM25} \cite{Robertson:2009:PRF:1704809.1704810} che è uno schema di pesatura basato sul \textit{modello probabilistico}. Questo schema è il risultato di uno studio puramente euristico.

\subsection{Metodi di ranking esogeno}
Uno dei metodi esogeni è \textit{PageRank} descritto in \cite{ilprints422}. PageRank usa le informazioni portate dai link in entrata (\textit{inlink}) per determinare un punteggio globale di importanza di una pagina. Esso assume che esiste un legame tra numero di \textit{inlink} di una pagina \textit{p} e la popolarità della pagina \textit{p}. Il concetto fondamentale dietro \textit{PageRank} è che una pagina è importante se moltre altre pagine importanti puntano ad essa. Questo concetto è mutualmente rinforzante ovvero l'importanza di una certa pagina influenza ed è influenzata dall'importanza delle altre pagine \cite{ilprints646}. In dettaglio \textit{PageRank} è basato sulla passeggiata naturale del grafo del web \textit{G}. Più precisamente, la passeggiata viene pertubata nel seguente modo: fissato un parametro \(\alpha\) tra \(0\) e \(1\), a ogni passo con probabilità \(\alpha\) si segue un arco uscente, e con probabilità \(1- \alpha \) si sceglie un qualunque altro nodo del grafo utilizzando una 
qualche 
distribuzione \(v\), detta
vettore di preferenza (per esempio, uniforme). Assumendo che non esistano pozzi, la catena è quindi rappresentata dalla combinazione lineare:
\begin{equation}
 \alpha G + (1 - \alpha) 1 v^T
\end{equation}
dove \(G\) è la matrice della passeggiata naturale su \(G\). Il fattore \(\alpha\) è detto fattore di attenuazione di norma è impostato a un valore di 0,85.

Un altro metodo usato per il ranking delle pagine è \textit{HITS (Hyperlink-Induced Topic Distillation)} introdotto in \cite{Kleinberg:1999:ASH:324133.324140}. Differentemente da \textit{PageRank} esso assegna due punteggi di importanza a ogni pagina: uno di \textit{hubbines} e uno di \textit{autorevolezza}. L’intuizione dietro a HITS è che invece di un singolo punteggio di importanza esista un concetto di pagina \textit{autorevole}, cioè pagina con contenuto pertinente e interessante, e di \textit{hub}, cioè pagina contenente numerosi collegamenti a pagine autorevoli. I due concetti si rinforzano \textit{mutuamente}: una pagina autorevole è puntata da molte pagine centrali, e una buona pagina centrale punta a molte pagine autorevoli.

Questo approccio considera che nel web ci sono due tipi di pagine: quelle che contengono dei contenuti per un determinato argomento (\textit{authoritative}) e quelle che contengono tanti link a delle pagine \textit{authoritative} che sono chiamate pagine \textit{hub}. Le pagine \textit{hub} sono utili per scoprire le pagine \textit{authoritative} \cite{Manning:2008:IIR:1394399p474}.

L'algoritmo parte da un sottografo del web ottenuto a partire da un'interrogazione. La selezione del sottografo può essere fatta in vari modi, un modo è quello di prendere un certo insieme di risultati  ottenuto da un motore di base e generare un sottografo sulla base di una query e delle pagine che puntano a quelle ottentute dalla query. Per questo sottoinsieme di pagine otteniamo un matrice di adicenza \(A\). I punteggi di \textit{hub} e \textit{authority} per tutte le pagine del sottoinsieme possono essere formalizzate dalla seguente coppia di equazioni:
\begin{equation}
 \left\{
 \begin{array}{cc}
    \stackrel{\rightarrow}{a} \: = \: A^T \stackrel{\rightarrow}{h}\\
    \stackrel{\rightarrow}{h} \:= A \: \stackrel{\rightarrow}{a}
 \end{array}
 \right .
 \label{eq:hubat}
\end{equation}
Può essere dimostrato che la soluzione per il sistema di equazioni \ref{eq:hubat} dopo una serie iterativa di calcoli converge rispettivamente al principale autovettore di \(AA^T\) e \(A^TA\) \cite{Manning:2008:IIR:1394399p474}\cite{Spirin:2012:SWS:2207243.2207252}.

\section{Web spam}
Con il termine web spamming si fa riferimento a tutti i metodi che tentano di manipolare gli algoritmi di ranking dei motori di ricerca per aumentarne il valore di alcune pagine rispetto ad altre \cite{ilprints646}.
Dato il numero esorbitante di pagine che vengono create e pubblicate sul web, gli utenti competono per far comparire le proprie pagine tra le prime dei risultati di una query.
Il fenomeno dello spamming o spamindexing ricade sulla qualità delle ricerche causando diversi problemi: indicizzazione di pagine che non sono utili, aumento del costo delle operazioni di query, malware e reindirizzamento verso contenuto per adulti ed infine che gli utenti che si sentono motivati ad utilizzare altri motori di ricerca,\cite{Spirin:2012:SWS:2207243.2207252}.

L'obbiettivo del motori di ricerca è di ottenere ottimi risultati per identificare tutte le pagine web che sono rilevanti per una specifica query e presentarle secondo l'importanza che esse hanno. Di norma la rilevanza viene misurata attraverso la similarità testuale tra la query e le pagine mentre l'importanza è definita come la popolarità globale della pagina e a volte è inferita dalla struttura dei link \cite{ilprints646}. Ci sono due categorie di tecniche associate al web spam \cite{ilprints646}:
\begin{itemize}
\item \textbf{tecniche boost} che cercano di far avere più importanza o rilevanza a delle pagine
\item \textbf{tecniche hiding} che sono metodi per nascondere le tencinche di boost all'utente dal browser, anche se alcuni autori incorporano queste tecniche facenti parte delle tecniche di boost
\end{itemize}

\subsection{Tecniche di boost}
Le tecninche di boosting si dividono in: \textit{Term Spamming} e \textit{Link Spamming}. Con l'avvento degli algoritmi di ranking basati sulla struttura del grafo il \textit{Term Spaming} è stato trascurato. In figura \ref{fig:tassonomiaTecnicheBoost} è specificata la tassonomia delle tecniche boost \cite{ilprints646}.
\begin{figure} 
 \centering
 \includegraphics[width=10cm]{immagini/tassonomiaTecnicheBoost}
 \caption{Tassonomia delle tecniche boost}
 \label{fig:tassonomiaTecnicheBoost}
\end{figure}

\subsection{Term Spamming}
Nel valutare la rilevanza testuale i motori di ricerca considerano dove i termini di una query compaiono in una pagina. Il tipo di punto all'interno della pagina è chiamato \textit{campo}. I più comuni campi di testo per una pagina \textit{p} sono: il body della pagina, il titolo, i meta tag nell'header HTML e l'URL della pagina. Inoltre viene considerato anche come \textit{campo}, il testo delle ancore (il tag \textit{a}) associate all'URL che puntano alla pagina \textit{p} dato che descrivere molto bene il contenuto della pagina . I campi di testo  di \textit{p} sono utilizzati per determinare la rilevanza di \textit{p} rispetto ad una query (alcune volte i campi vengono pesati sulla base della loro importanza) e perciò chi fa \textit{term spamming} utilizza tecniche di pesatura dei contenuti dei campi di testo in modo tale da aumentare l'efficacia dello spam \cite{ilprints646}. Le tecninche di spamming possono essere raggruppate in base ai \textit{campi} di testo dove viene fatto spamming. In base a questo distinguiamo \cite{ilprints646}:
\begin{itemize}
\item \textit{Body Spam}. In questo caso lo spam è nel corpo del documento. Questo è lo spam più diffuso.
\item \textit{Title Spam}. Molti motori di ricerca danno molta importanza ai termini che compaiono nel titolo. Quindi ha senso includere termini di spam all'interno del titolo della pagina.
\item \textit{Meta Tag Spam}. I tag che compaiono nell'header sono molto frequentemente soggetti a spam. Per questo i motori di ricerca danno poca importanza a questi campi o non li considerano. Di seguito viene mostrato un esempio di spam.
\begin{lstlisting}[frame=trbl,postbreak=\space, breakindent=5pt, breaklines]
 <meta name="keyword" content="buy, cheap, cameras, lens, accessories, nikon, canon">
\end{lstlisting}
\item \textit{Anchor Text Spam}. I motori di ricerca assegnano un peso maggiore al testo nelle ancore perché pensano che esse contengano un riassunto del contenuto della pagina. Perciò testo di spam è incluso nel testo delle ancore dei collegamenti HTML di una pagina. In questo caso lo spamming non viene fatto sulla pagina che si vuole far avere un rank più alto ma sulle pagine che puntano ad essa.
\begin{lstlisting}[frame=trbl,postbreak=\space, breakindent=5pt, breaklines]
<a href="target.html">free, great deals, cheap, inexpensive, cheap, free</a>
\end{lstlisting}
\item \textit{URL Spam}. Alcuni motori di ricerca dividono l'URL delle pagine in un insieme di termini che sono usati per determinare la rilevanza di una pagina. Per sfruttare questo metodo di ranking, gli spammers creano lunghi URL che includono una grande sequenza di termini spam, un esempio può essere: \textit{buy-canon-rebel-20d-lens-case.camerasx.com}.
\end{itemize}
Queste tecniche possono essere utilizzate insieme o separatamente. Un altro modo per raggruppare queste tecniche si basa sul tipo di termini che vengono utilizzati nei campi di testo \cite{ilprints646}:
\begin{itemize}
\item Ripetizione di uno o più specifici termini.
\item Inclusione di molti termini generici per creare pagine rilevanti per molte query.
\item Intreccio di vari termini all'interno della pagina.
\item Creazione di frasi di senso compiuto per l'elaborazione di contenuti generati velocemente attraverso la concatenazione di frasi da fonti diverse.
\end{itemize}

\subsection{Link Spamming}
In \cite{ilprints646} viene definito che per uno spammer ci sono tre tipi di pagine nel Web: inaccessibili, accessibili(blog) e proprietarie (fig. \ref{fig:tipologiaPagine}). Le inaccessibili sono quelle che uno spammer non può modificare. Le accessibili sono pagine gestite da altri ma che possono essere modificate livemente dallo spammer attraverso l'immissione di un post in un forum o in un blog o portali di questo genere. Le proprietarie sono pagine degli spammer di cui hanno il pieno controllo. Il gruppo di pagine proprietarie è chiamato \textit{spam farm}.
\begin{figure} 
 \centering
 \includegraphics[width=10cm]{immagini/tipologiaPagine}
 \caption{Tipi di pagine nel web per uno spammer}
 \label{fig:tipologiaPagine}
\end{figure}

Di norma i motori di ricerca utilizzano due algoritmi per aumentare l'importanza basandosi sulle informazioni dei link: PageRank e HITS e sulla base di questi due tipi di algoritmi vengon definite due categorie principali di link spam: \textit{outgoing link spam} e \textit{incomign link spam}. L'\textit{Outgoing link} è uno dei metodi più facili da implementare in quanto basta aggiungere dei link nella propia pagina, ad altre pagine che sono considerate buone, sperando di poter aumentare il  punteggio di \textit{hub}. Per la ricerca di link da includere nella pagina per cui si vuole incrementare il punteggio di \textit{hub} si possono utilizzare delle directory che contengono liste di siti come DMOZ o Yahoo!. Queste directory organizzano i contenuti web in contenuti e in liste di siti relativi. Per quanto riguarda \textit{Incoming link}, ci sono diverse strategie che si possono adottare in modo tale da avere un numero elevato di link in entrata \cite{ilprints646}:
\begin{itemize}
\item \textit{Honeypot}: ovvero si creano un insieme di pagine che hanno un contenuto interessante (un esempio può essere una documentazione Linux) ma che hanno link nascosti alla pagina o alle pagine per cui si deve aumentare il valore di rilevanza.  
\item \textit{Infiltrarsi in una directory web}: molte directory web permettono ai webmasters di postare links ai loro siti che hanno lo stesso contenuto.
\item \textit{Postare link nei blog, forum e wiki}: includere URL a pagine di spam come parte di un commento.
\item \textit{Scambio di link}: scambiare link con altre pagine di spam. Questa è una pratica comune tra chi fa spam ed esistono blog completamente finalizzati all'incontro di spammer per lo scambio dei link.
\item \textit{Comprare domini scaduti}: quando un dominio scade ci sono delle pagine che puntano ancora ad esso. Comprando questi domini e riempirli di spam ha dei vantaggi per la rilevanza che si acquisice dai link che puntano ancora ad essa	.
\item \textit{Creare una spam farm}: con l'abbassamento dei costi si possono costruire delle spam farm che hanno come obbietivo di aumentare la rilevanza di un pargina spam detta \textit{target page}, un esempio è mostrato in fig. \ref{fig:spamfarm}. Molte volte si utilizzano tecninche come \textit{honeypot}. In questo caso il valore di page rank aggregato delle pagine è propagato alla pagina target. Una delle forme più aggressive di honeypot è l'\textit{hijacking} \cite{Spirin:2012:SWS:2207243.2207252}, dove gli spammers prima attaccano un sito con una buona reputabilità e poi usano questo come parte della loro link farm.
\end{itemize}
\begin{figure} 
 \centering
 \includegraphics[width=10cm]{immagini/spamfarm}
 \caption{Esempio di una spamfarm}
 \label{fig:spamfarm}
\end{figure}

\subsection{Tecninche di hiding}
Le tecninche di hiding si possono classificare in: \textit{content hiding}, \textit{cloaking}, \textit{redirection} (fig. \ref{fig:tecnicheHiding}) \ref{ilprints646}.
\begin{figure} 
 \centering
 \includegraphics[width=10cm]{immagini/tassonomiaHiding}
 \caption{Tecniche di hiding}
 \label{fig:tecnicheHiding}
\end{figure}
Nel \textit{Content hiding} i termini o link di spam possono essere nascosti quando il browser visualizza una pagina. Una tecnica è quella di utilizzare lo stesso colore per i termini e lo sfondo. Mentre per i link basta non inserire il testo all'interno delle ancore che indirizzano a una pagina. Un'altra tecnica è quella di utilizzare degli script per nascondere il contenuto. Il \textit{Cloaking} sfrutta il fatto che è facile identificare quando la richiesta di una pagina è fatta da un crawler o da un browser. Perciò questa tecnica dato un URL, il server spam restituisce un documento HTML diverso a seconda se la richiesta è fatta da un crawler o da un browser. Quindi vengono distribuiti due contenuti diversi in base se la richiesta al server spam è fatta da un crawler o da un browser. La rilevazione di un crawler può essere fatta in due modi: o si mantiene in memoria una lista di indirizzi di crawler oppure attraverso l'header della richiesta HTTP andando a vedere il campo user-agent se questo è diverso dai più comuni browser allora può essere un crawler.

\textit{Redirection}. Un altro modo è quello di reindirizzare il browser ad un altro URL appena la pagina è caricata.

\subsection{Click Spamming}
Dal momento che i motori di ricerca utilizzano i dati sul flusso di click per regolare le funzioni di ranking, gli spammers generano clik fraudolenti per manipolare il comportamento di queste funzioni in modo tale da fare avere un migliore rank i loro siti. Vengono fatte delle query e si clicca sulla pagina che si vuole aumentare il rank. Questo viene fatto in modo automatico attraverso script che girano su diverse macchine per non fare sospettare di tale comportamento.


