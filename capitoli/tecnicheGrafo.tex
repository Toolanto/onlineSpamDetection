\chapter{Tecniche basate sul grafo}
\lstset{basicstyle=\small\ttfamily,keywordstyle=\color{black}\bfseries,commentstyle=\color{darkgray},stringstyle=\color{black},showstringspaces=true}
In questo capitolo verrano presentate le tecniche presenti in letteratura che si avvalgono del grafo derivato dalla fase di crawling per rilevare lo spam. In particolare tali tecniche fanno uso del grafo del web ricavato dai collegamenti ipertesuali tra le pagine. Il web, quindi, può essere rappresentato come un grafo diretto \textit{G = (V,E)}, dove \textit{V} è l'insieme delle pagine e rappresentano i nodi del grafo mentre \textit{E} è l'insieme dei link diretti tra le pagine. Il grafo può essere astratto e rappresentato da una matrice di transizione cosi formata:
\begin{equation}
T(p,q)=\left \{
\begin{array}{cc}
0 & if(q,p) \in E\\
1/\omega(q) & if(q,p) \in E
\end{array}
\right .
\end{equation}
dove \(\omega(p)\) è il grado di link in uscita della pagina \(p\).
Possiamo anche definire la matrice di transizione inversa U:
\begin{equation}
U(p,q)=\left \{
\begin{array}{cc}
0 & if(p,q) \in E\\
1/l(q) & if(p,q) \in E
\end{array}
\right .
\end{equation}
dove \(l(q)\) è il grado di link in ingresso della pagina \(q\).
\section{Metodi classici per identificare lo spam web usando il grafo}
Uno dei primi metodi adottati per identifiare lo spam web usando il grafo è \textit{Trustrank} \cite{Gyongyi:2004:CWS:1316689.1316740}. \textit{Trustrank} fa uso di un insieme di pagine di partenza \(S\) che sono valutate da degli esperti e che vengono classificate in due sottoinsiemi: pagine non spam \(S^+\) e pagine spam \(S^-\); questo fase è chiamata funzione \textit{Oracle}. Per determinare le pagine non spam senza invocare la funzione \textit{Oracle} su tutto il grafo derivato dalla fase di crawling, viene fatta un'assunzione empirica chiamata \textit{isolazione approssimata dell'insieme delle pagine buone} la quale afferma che le pagine non spam raramente punteranno a quelle spam perché gli sviluppatori di pagine non spam hanno poco interesse nel linkare pagine spam almeno che non vengano ingannati tramite ad esempio l'uso di tecniche come l'\textit{honeypot}. Dato un numerto limitato di chiamate della funzione \textit{Oracle} sul seed set di partenza e sfuttando l'assunzione fatta precendentemente 
viene definita una funzione, denominata come \textit{funzione di verità ignorante \(T_0\)}, per ogni pagina pagina \(p\) del grafo:
\begin{equation}
T_0(p)=\left\{
\begin{array}{ccc}
O(p) & if & p\in S \\
1/2 & altrimenti
\end{array}
\right .
\end{equation}
dove la funzione \(O\) è la funzione \textit{Oracle}. Dal momento che le pagine buone dovrebbero puntare ad altre pagine buone assegnamo 1 a tutte le pagine che possono essere raggiunte da una pagina in \(S^+\) in \(M\) step. La funzione di verità \(T_M\) è definita come:
\begin{equation}
T_M(p)=\left\{
\begin{array}{ccccc}
O(p) & if & p\in S \\
1 & if & p \not\in S & and & \exists q\in S^+:q\rightarrow_M p \\
1/2 & altrimenti
\end{array}
\right .
\end{equation}
Il percorso  dalla pagina \(q\) a \(p\) nell'equazione non comprende pagine spam incluse nell'insieme \(S^-\).

Il problema della funzione di verità \(T_M\) è che non esiste la sicurezza che le pagine raggiungibili da pagine buone siano effetivamente della stessa carattesistica. Infatti più lontana una pagina \(p\) si trova dal seed set \(S^+\) minore è la certezza che quella pagina sia buona. Un modo per non incorrere in questo errore è ridurre il valore della funzione di verità ogni qual volta ci si allontana dal seed set \(S^+\).

In figura \ref{fig:trustrank1} è possibile vedere in dettaglio l'algoritmo. L'algoritmo calcola il valore di verità di ogni pagina dell'intero grafo. I valori di input sono il grafo descritto dalla matrice di transizione \(T\) e il numero di pagine \(N\) e i parametri di controllo dell'esecuzione: \(L\) il numero di chiamate della funzione \(Oracle\) e \(\alpha_b\) il fattore di decadimento per il calcolo di \textit{Pagerank} ed infine \(M_b\) il numero di iterazioni per il calcolo di \textit{Pagerank}. Al primo passo viene invocata la funzione \textit{SelectSeed()} calcola l'insieme delle pagine con il relativo rank di rilevanza per essere incluse nel seedset di partenza. Nel secondo punto la funzione \(Rank(x,s)\) ordina gli elementi di \(x\) in modo decrescente sulla base dello score di \(s\). Il punto tre invoca la funzione \textit{Oracle} su \(L\) pagine. I valori del vettore \(d\) che corrispondono alle pagine buone del seed sono imposate a 1. Nel punto (4) il vettore viene normalizzato in modo tale 
che la somma faccia 1. Infine al punto (5) viene calcolato \textit{Trustrank} usando \textit{Pagerank} personalizzato dal vettore \(d\) che rimpiazza la distribuzione uniforme. Dall'algoritmo si nota che \textit{Trustrank} è una versione modificata di \textit{Pagerank} dove il vettore di teletrasporto è il seed set \(S^+\) calcolato al punto 3 e 4.
\begin{figure}
\centering
\includegraphics[width=8cm]{immagini/trustrank/trustrank}
\caption{Algoritmo di trustrank}
\label{fig:trustrank1}
\end{figure}

Un altro algoritmo che è stato progettato per identificare lo spam usando come input il grafo delle pagine web è \textit{Anti-Trust Rank} \cite{Krishnan06webspam}. Questo algoritmo sfrutta la stessa intuizione di \textit{Trustrank} dell'isolamento approssimato cioè che pagine buone molto raramente punteranno a pagine malevoli;  quindi si popola un seed set formato da pagine spam e si propaga la funzione Anti Trust (che sarebbe la funzione di verità di Trustrank) sul grafo trasposto con l’obbiettivo di rilevare le pagine spam, le quali successivamente possono essere filtrate da un motore di ricerca. Più precisamente a differenza per quanto avviene in \textit{Trustrank} dove la funzione \textit{Trust} è propagata dal seed set composto da pagine non spam lungo tutto il grafo, in \textit{Anti-Trust Rank} la funzione (in questo caso la funzione \textit{Anti Trust}) è propagata nella direzione inversa ai link in entrata ad ogni pagina del grafo, partendo da un insieme di pagine del seed set composto da pagine spam.
 L'obbiettivo è assegnare un rank maggiore alle pagine spam e successivamente eliminarle dalle ricerche o usando un valore di soglia oppure ritornando le \(n\) pagine che hanno valore di \textit{Anti-Trust Rank} più alto.
 
\textit{Trustrank} e \textit{Anti-Trust rank} sono ottimi algoritmi per identificare lo spam, ma hanno il problema che l’insieme seed usato potrebbe non essere sufficientemente rappresentativo per coprire bene tutti gli argomenti del web. Un modo naturale di ottenere una grande copertura del web è usare gli argomenti delle pagine come segnale di ingresso: invece di usare un singolo valore di \textit{trustrank} per un sito, in \cite{Wu:2006:TTU:1135777.1135792} gli autori propongono di calcolare \textit{trustrank} per i differenti argomenti di ogni sito. L'algoritmo consiste nel partizonare il seed set sulla base dei vari argoementi che esso contiene e usare ognuna di queste partizioni come seed set per calcolato il valore di \textit{trustrank} per ogni pagina.

Un altro metodo per l'identificazione di pagine spam è descritto in \cite{Caverlee:2007:CWS:1281100.1281124}. Questo metodo separa la credibilità di una pagina dalla credibilità del link per quella pagina al contrario di \textit{pagerank} che è manipolabile tramite tecniche come \textit{hoenypot}. La credibilità viene definita in termini di credibilità \textit{k-scope}. Data una funzione \(C\) essere una funzione di credibilità che istantaneamente valuta la qualità di un link di un pagina \(p\) al tempo \(t\), un valore di \(C(p,t)=0\) indica che \(p\) non è credibile mentre \(C(p,t)=1\) indica che \(p\) è credibile. Dato  un percorso in un grafo diretto \(G\) dalla pagina \(p\) alla pagina \(q\) essere la sequenza di nodi: \(path(p,q)=(n_0,n_1,...,n_j)\) dove \(p=n_0, q=n_j\) tale che esiste un arco diretto tra nodi successivi nel percorso \(n_i,n_{i+1}\in L\) per \(0\leq i \leq j-1\), diciamo che un percorso  in un grafo diretto \(G\) dalla pagina \(p\) alla pagina \(q\) è un \textit{bad path} se la pagina 
di destinazione è una pagina spam \(q\in P_b\) (dove \(P_b\) è l'insieme delle pagine spam) e nessuna altra pagina nel percorso è una pagina spam. \(path(p,q)=(n_0,n_1,...,n_j)\) e \(q\in P_b\) e \(n_i\not\in P_b (0\leq i\leq j-1)\). La probabilità che una camminata casuale passi, lungo un percorso di lunghezza \(k\), da una pagina \(p\) è denotata con \(Pr(path_k(p))\) ed è determinata con i pesi degli archi per ogni hop nel percorso:
\begin{equation}
 PR(path_k(p))=\prod_{i=0}^{k-1}w(n,n_{i+1})
\end{equation}
Quindi credibilità \textit{k-scope} di una pagina  è definita in termini di probabilità che una camminata casuale eviti le pagine spam dopo aver superato \(k\) hop dalla pagina di origine. La credibilità \textit{k-scope} di una pagina \(p\) al tempo \(t\), denotata con \(C_k(p,t)\) è definita come segue:
\begin{equation}
 C_k(p,t)=1-\sum_{l=1}^k\left (\sum_{path_l(p)\in BPath_l(p)}Pr(path_l(p))\right )
\end{equation}
Nel caso \(p\in P_b\) allora \(C_k(p,t)=0\). Nel caso in cui non ci siano pagine spam all'interno di \(k\) hop di pagine allora \(p\) è credibile con un valore \(C_k(p,t)=1\) se lei è un pagina spam o nel caso in cui tutti i percorsi originati da \(p\) colpiscono una pagina \(p\) all'intenro di \(k\) hop, allora \(p\) no è credibile \(C_k(p,t)=0\). Ma dato che non che non è possibile avere tutto il grafo e non c'è nessuna sicurezza sulla conoscenza totale dei nodi spam è stato introdotto il concetto di cerdibilità tunable k-Scope, la quale aumenta il calcolo della credibilità k-scope includendo un fattore di penalità di credibilità. GLi obbiettivi sono approssimare al meglio la credibilità k-scope sotto limiti reali e capire come parametri differenti protrebbero influire sulla qualità delle varie funzioni usate. Sia \(G=(P,L)\) essere un grafo diretto, k il raggio massimo di camminata e \(\gamma(p)\) il fattore di penalità di credibilità di una pagina \(p\in P\) dove \(0\leq \gamma(p)\leq 1\). Definiamo la 
credibilità tunable k-scope di una pagina \(p\), denotata con \(C_k(p)\), in due fasi, quando \(p \not \in P_b\):
\begin{equation} 
C_k(p)=\left ( 1 -\sum_{l=1}^k \left ( \sum_{path_l(p)\in BPath_l(p)} Pr(path_l(p)) \right ) \right ) \cdot\gamma(p)
\end{equation}
e quando \(p\in P_b\) allora: \(C_k(p)=0\).

Oltre al metodo per definire la credibilità di un link gli autori in \cite{Caverlee:2007:CWS:1281100.1281124} propongono un algoritmo, denominato \textit{CredibleRank} di ranking basato sulla credibilità. \textit{CredibleRank} definisce che la qualità di una pagina è determinata da due criteri: la qualità delle pagine che puntano ad essa e la credibilità di ogni pagina puntata. Un link da un alta-qualità/alta-credibilità conta più di un link da alta-qualità/bassa-credibilità. Definendo con \(In(p)\) l'insieme di pagine che puntano a \(p\). Calcoliamo \textit{CredibleRank} \(r_c(p)\) per una pagina \(p\)
\begin{equation}
r_c(p)=\sum_{q\in In(p)}C(q)\cdot r_c(q)\cdot w(q,p)
\end{equation}
Questa formula dice che il valore di \textit{CredibleRank} di una pagina \(p\) è determinato dalla qualità \(r_c(q)\) e dalla credibilità dei link \(C(q)\) delle pagine che la puntano cosi come la forza del link \(w(q,p)\).

Oltre ai metodi descritti in \cite{Leon-Suematsu:2011:WSD:2052138.2052339} è rappresentato un metodo per rilevare lo spam composto da tre fasi: decomposizione del grafo in sottografi densi e calcolo delle feature per ogni sottografo quali: statistiche base (conteggio, somma, media) della lunghezza degli URL, lunghezza del percorso, lunghezza del hostname, distanza hostname e feature basate sulla struttura dei link; successivamente si fa uso di un classificatore SVM  per identificare i sottografi composti da spam; infine per aumentare il numero di host spam e non spam vengono propagati simultaneamente \textit{trustrank} e \textit{anti-trust rank}, dagli host che siamo sicuri siano non spam e spam agli host vicini, per valorizzare gli host non spam e penalizzare quelli di spam.

Un altro metodo per rilevare lo spam combina feature basate sui link e quelle basate sul contenuto \cite{Castillo:2007:KYN:1277741.1277814}. L'algoritmo prevede l'implementazione di un classificatore automatico che combina un insieme di feature basate su link e contenuto. Dato che i link tra le pagine non sono piazzati in modo casuale ovvero pagine simili tendono a linkarsi tra di loro più frequentemente di pagini diverse, si può sfruttare tale meccanismo per rilevare le pagine spam, perché tali pagine tendono a raggrupparsi in cluster d. Una spiegazione per questo fenomeno è che le pagine spam utilizzano delle tecniche per aumenteare il rank basato sui link attraverso le link farm. Gli autori assumono che gli host che sono ben collegati tra di loro sono molto probabilmente della stessa classe: spam o non spam.

\section{Metodi per identificare spam farm}
Per riconoscere una spam farm si parte dal presupposto che i nodi della spam farm avranno dei link uscenti verso delle pagine target \textit{t} per aumentarne il rank. In \cite{Gyongyi:2006:LSD:1182635.1164166} per identificare le spam farm viene introdotta una misura, denominata \textit{spam mass}, dell'impatto dello spam (basandosi sulla struttura del grafo) sul rank di una pagina. Le pagine target delle spam farm allora riceveranno, oltre ad un alto valore di \textit{pagerank}, un alto valore di \textit{spam mass} mentre le pagine non spam anche se hanno un alto valore di \textit{pagerank} riceveranno un basso valore di \textit{spam mass}. Un modo per stimare la \textit{spam mass} per ogni nodo del grafo è partire dal presupposto che le il web può essere partizionato in nodi non spam \(V^+\) e nodi spam \(V^-\) e la loro unione forma il grafo del web. Per una data partizione \(\{V^+,V^-\}\) di \(V\) e per dei nodi \(x\) il pagerank di \(x\) è la somma dei contribbuti di nodi non spam e dei nodi spam. 
Quindi vengono definite due misure di \textit{spam mass}:
\begin{itemize}
 \item La \textit{spam mass assoluta} di \(x\), denotata con \(M_x\), è il \textit{pagerank} che \(x\) riceve dai nodi spam è che uguale a:
 \begin{equation}
   M_x=q_x^{V^-}
 \end{equation}
dove \(q_x^{V^-}\) è appunto il \textit{pagerank} di \(x\) derivato dai nodi spam.
 \item La \textit{spam mass relativa} di \(x\), denotata da \(m_x\), è la frazione del \textit{pagerank} di \(x\) dovuto dal contribbuto dei nodi di spam cioè: 
 \begin{equation}
   m_x=q_x^{V^-}/p_x
 \end{equation}
dove \(q_x^{V^-}\) è il \textit{pagerank} di \(x\) derivato dai nodi spam e \(p_x\) il \textit{pagerank} derivato da tutti i nodi.
\end{itemize}
Dal momento che non è possibile conoscere le proprietà (spam o non spam) per tutti i nodi del grafo ma solo un sottoinsieme di nodi buoni \(\tilde{(V)}^+\) le misure precedenti vengono calcolate nel seguente modo:
\begin{itemize}
 \item la stima assoluta di \textit{spam} mass di un nodo \(x\) è:
 \begin{equation}
 \tilde{M}_x=p_x-p'_x
\end{equation}
\item la stima relativa di \textit{spam mass} di \(x\) è:
 \begin{equation}
 \tilde{m}_x=(p_x-p'_x)/p_x=1-p'_x/px
\end{equation}
\end{itemize}
dove \(p=PR(v)\) è il \textit{pagerank} dei nodi basato su una distribuzione uniforme mentre \(p'=PR(v^{\tilde{V}^+})\) è \textit{pagerank} basato sull'insieme \(\tilde{(V)}^+\)  con una distribuzione di salto \(v^{\tilde{V}^+}\). Nel caso in cui si conoscesse \(\tilde{V}^-\) lo \textit{spam mass} può essere stimato con \(M=PR(v^{\tilde{V}^-})\). Mentre se si conoscerro entrambi i sottoinsiemi \(V^+, V^-\) la stima dello \textit{spam mass} può essere fatta attraverso \((\tilde{M}+\tilde{M})/2\). Perciò è possibile utilizzare un valore di soglia tramite la quale una pagina è considerata facente parte di una spam farm se il valore di \textit{spam mass} supera la soglia.

Le pagine all'interno delle spam farm sono densamente connesse tra di loro e molte pagine all'interno delle spam farm hanno molti link in entrata e uscita. Quindi  impostando queste pagine come seed set, per ogni nuova pagina, essa può può fare parte della spam farm se questa ha molti link in entrata e uscita, da e per, il seed set. Allora si può allargare il seed set aggiungendo la nuova pagina. Questo processo può essere iterato. Il processo terminerà quando nessuna altra pagina potrà essere aggiunta. Il metodo per identificare le spam farm è descritto in questo modo è descritto in \cite{Wu05identifyinglink}. Per decidere se una pagina deve fare parte di un seed set, si parte dall'assunzione che le pagine all'interno della link farm normalmente hanno molti nodi in comune tra l'insieme dei link in entrata e quello dei link in uscita. Se ci sono solo uno o due nodi in comune non etichettiamo queste pagine come pagine problematiche ma se ci sono molti nodi in comune è probabile che queste pagine facciano 
parte di una spam farm, l'algoritmo è presentato in dettaglio in figura \ref{fig:linkfarm1}. 
\begin{figure}
\centering
\includegraphics[width=8cm]{immagini/linkfarm/immagine1.png}
\caption{Algoritmo di ricerca dei seed set}
\label{fig:linkfarm1}
\end{figure}
Se il numero di incoming link in comune o outgoing link in comune è  uguale o maggiore a una soglia \(T_{IO}\) allora le pagine sono etichettate come spam.  Dall'intuizione che se una pagina punta a un insieme di pagine cattive è probabile che anche essa sia cattiva viene allargato il seed set usando un altra soglia \(T_{PP}\) per giudicare una pagina: se il numero di outgoing link a pagine spam è uguale o supera la soglia, la pagina sarà giudicata spam e perciò facente parte del seed set della spam farm. Il metodo descritto è formalizzato nell'algoritmo di \textit{ParentPenality} in figura \ref{fig:linkfarm2}.
\begin{figure}
\centering
\includegraphics[width=8cm]{immagini/linkfarm/immagine2.png}
\caption{Algoritmo per aumentare il seedset}
\label{fig:linkfarm2}
\end{figure}
Una volta trovate le pagine spam bisogna utilizzare queste informazioni per il ranking. Un modo è quello di eliminare queste pagine direttamente dal grafo del web, un altro modo può essere quello di penalizzare i link invece che le pagine, facenti parte della spam farm, con un fattore di decadimento o infine potrebbe essere utile eliminare direttamente i link che fanno parte della spam farm.

\section{Metodi per migliorare la classificazione}
Oltre ai metodi descritti fino adesso sono stati sviluppati altre tecniche con l'obbiettivo di migliorare la fase di classificazione invece di cercare a migliare la rilevazione delle pagine spam utilizzando nuove feature.

In \cite{Gan:2007:IWS:1244408.1244412} viene presentato un metodo per migliorare la classificazione che utilizza un classificatore di base per etichettare le pagine e successivamente venngono definite delle euristiche, basate sulla tipologia dei nodi vicini a un nodo \(v\), per determinare se il nodo \(v\) doverebbe essere rietichettato basandosi sulla basa della prima classificazione o sull'infromazione portata dai nodi vicini. Gli autori ipotizzano che per un sito, la struttura dei vicini è un buon indicatore,  per classificarlo in  spam o non spam. In particolare sono interessanti alcune distribuzioni delle proprietà dei vicini:
\begin{itemize}
 \item la distribuzione dello spam in entrata:  in figura \ref{img:gan1} viene rappresentata la distribuzione dello spam in entrata: ogni sito andrà a finire in uno dei settori sull'asse \(x\) in base alla frazione di nodi spam tra i sui vicini in entrata. L'asse \(y\) rappresenta la percentuale di spam/non spam all'interno del settore. Una grande porzione di siti spam ha molti vicini che sono spam. 
 \begin{figure}
 \centering
\includegraphics[width=8cm]{immagini/gan/immagine1.png}
\caption{Distribuzione dello spam in entrata}
\label{img:gan1}
\end{figure}
\item la distribuzione dello spam in uscita: in figura \ref{img:gan2} viene osservato una distribuzione simile a quella per lo spam in entrata.
 \begin{figure}
 \centering
\includegraphics[width=8cm]{immagini/gan/immagine2.png}
\caption{Distribuzione dello spam in uscita}
\label{img:gan2}
\end{figure}
\item distribuzione entrante pesata:vegono esaminati gli in-lin  pesati con pagerank (figura \ref{img:gan3}).
 \begin{figure}
 \centering
\includegraphics[width=8cm]{immagini/gan/immagine3.png}
\caption{Distribuzione entrante pesata}
\label{img:gan3}
\end{figure}
\end{itemize}
Per rietichettare i nodi vengono cambiate le etichette che sono assegnate dal primo classificatore  sulla base dei nodi vicini, in particolare
prima viene definita un etichetta per i nodi vicini di un sito, con un certo livello di confidenza, dopo viene confrontata questa etichetta con la prima (assegnata dal classificatore) se le due etichette sono molto diverse tra loro e l'etichette dei vicini sono moltoo affidabili in termini di confidenza 
allora l’etichetta del sito viene cambiata ooppure si può usare un altro classificatore che usa le euristiche definite prima.

Un altro metodo che fa uso della riassegnazione delle etichette associate a ogni nodo dopo una prima fase di classificazione per migliorare la classificazione è descritto in \cite{Geng:2008:IWS:1367497.1367685}. Tale metodo consiste nell'usare una strategia di riestrazione delle feature facendo delle analisi basate su clustering, propagazione e grafo dei nodi vicini. Il metodo è suddiviso in quattro fasi:
\begin{itemize}
 \item estrazione delle feature base
 \item prima calssificazione
 \item riestrazione delle feature
 \item seconda fase di calssificazione
\end{itemize}
Nelle prime due fasi vengono estratte le feature di base basate sul contenuto e sulla struttura dei link e viene fatta una prima classificazione; nella terza e quarta fase vengono estratte i nuovi tipi di feature e successivamente viene rieseguita la classificazione.

Il problema di molti algoritmi di classificazione è il riperimento di dati etichettati ovverò per avere un buon risultato nella classificazione i dati di training devono essere consistenti. Per risolvere questo problema in \cite{Geng:2009:LBS:1526709.1526920} viene proposto un algoritmo di apprendimento supervisionato per migliorare le performance di un classificatore. Tale algoritmo è basato sul tradizionale self training e sull'apprendimento dai link ovvero la dipendenza topologica, l'algoritmo è denominato \textit{Link-training}. L'algoritmo di apprendimento si può riassumere nei seguenti processi:
\begin{itemize}
 \item Prima viene istruito un classificatore con un piccolo dataset.
 \item Successivamente viene utilizzato il classificatore per categorizzare e assegnare un valore di spam (\textit{PS}) ai dati non etichettati; il calcolo di \textit{PS} avviene nel seguente modo:
 \begin{equation}
  PS(x)=\frac{P_{spam}(x)}{P_{spam}(x)+P_{normal}(x)}
 \end{equation}
 dove, \(P_{spam}(x)\) è la probabilità di \(x\) di essere un nodo spam.
 \item Il passo successivo consiste nell'assegnare a tutti i nodi il valore di spam calcolato.
 \item Per istruire il classificatore oltre al valore di spam di base viene calcolato anche quello relativo ai vicini (LS):
 \begin{equation}
LS(h)=\frac{\sum_{v\in N(h)}(PS(v)\times weight(h,v))}{\sum_{v\in N(h)}weight(h,v)}
 \end{equation}
dove \(v\) e \(h\) sono gli host, \(weight(h,v)\) è il peso dell'host \(h\) e \(v\) , \(weight(h,v\in {1,\log{(n)}})\), dove \(n\) è il numero di link tra i due nodi. \(N(h)\in inlink(h) \cup outlink(h)\), dove \(inlink(h)\) rappresenta l'insieme dei link in entrata di \(h\) e \(outlink(h)\) rappresenta l'insieme dei link in uscita di \(h\). 
Queste fasi del processo sono cicliche per un numero stabilito di iterazioni.
\end{itemize}

Gli algoritmi di spam detection basati sul grafo che viene ottenuto dalla fase di crawling, cercano di sfruttare le caratteristiche dei grafi per ottenere delle informazioni riguardo i nodi spam. Quasi tutti i metodi descritti si basano sulla stessa intuizione dell'algoritmo di \textit{trustrank} (quella realativa all'isolazione approssimata delle pagine buone) sfruttando tale intuizione per determinare le pagine spam. Ad esempio \textit{Anti-trust rank} sfrutta tale intuizione ma non andando a manipolare i link in uscità di ogni nodo ma i link dei nodi entranti. Altri metodi mentre si basano sulla ricerca delle spam farm mentre altri ancora si focalizzano sul miglioramento dei vari algoritmi di classificazione. Oltre ai metodi e feature descitte fino adesso ci sono altri metodi che utilizzano criteri diversi dal contenuto delle pagine web e dalla struttura del grafo per determinare le pagine spam, tali metodi sono nati per andare incontro alla crescita di nuovi tipi di spam web.




