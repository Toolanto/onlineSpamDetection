\chapter{Conclusioni}

Col crescere delle dimensioni del web, aumenta la difficoltà di una pagina di comparire tra i primi risultati di un motore di ricerca per una data quey. Questo fenomeno porta allo sviluppo di meccanismi di spam per manipolare gli algoritmi dei motori di ricerca al fine di ottenere un rank maggiore per una data pagina web. Per combattere questi meccanismi di spam sono state introdotte varie tecniche di spam detection, la maggior parte delle quali operanti offline.\\
L'obbiettivo di questa tesi è stato, dunque, analizzare le  varie tecniche di spam detection descritte in letteratura al fine di analizzarne  il comportamento e vagliare la possibilità di utilizzo di tali tecnhiche online. 

Le varie tecniche descritte in letteratura sono state classificate in macro gruppi sulla base del tipo di segnale utilizzato; si sono individuati tre gruppi:
\begin{itemize}
 \item tecniche basate sul contenuto;
 \item tecniche basate sul grafo del web;
 \item tecniche innovative che usano segnali differenti dalle prime due. 
\end{itemize}
Le prime utilizzano il contenuto di una pagina web \(p\) per determinare se la pagina \(p\) è spam oppure non spam; le seconde utilizzano il grafo delle pagine web, ricavato dai link tra le pagine, per determinare se la pagina è spam o non spam; le terze, invece, utilizzano tipi di segnali eterogenei per identificare la natura di una pagina web \(p\) (ad esempio analizzando specifici pattern comportamentali dell'utente).

Dalla fase di analisi delle tecniche presenti in letteratura si è evinto che le tecniche di spam detection basate sul contenuto e sul grafo sono quelle più studiate e utilizzate. I metodi di spam detection innovativi che utilizzano segnali eterogenei sono poco studiati ma riescono ad identificare tipi di spam che i metodi classifici non rilevano; all'interno di questo insieme i metodi che fanno uso di pattern comportamentali rendono il problema dell’identificazione dello spam scalabile, ovvero consentono di rilevare nuove tipologie di spam web senza ogni volta definire nuove feature. Questo indica che tali metodi protrebbero essere utilizzati singolarmente mantenendo delle buone prestazioni. 
Al termine del lavoro di analisi è emerso quindi che l'utilizzo complementare dei metodi classici (basati sul contenuto e basati sul grafo) e dei metodi innovativi è ottimale per l' identificazine di pagine spam, in modo tale da raffinare la rilevazione di tali pagine.

Al termine della fase di documentazione si è deciso di analizzare due algoritmi di spam detection operanti offline, quali \textit{TrustRank} e \textit{Anti-Trust Rank} durante la fase di crawling. Il razionale di tale analisi è stato quindi la valutazione dell'operabilita di tali algoritmi durante l'esecuzione online e il confronto delle prestazioni rispetto all'utilizzo convenzionale offline.

Dai test condotti si è evinto che \textit{TrustRank} e \textit{Anti-Trust rank} online approssimano abbastanza bene il loro comportamento offline. Nello specifico le prestazioni dipendono dalla dimensione del grafo, derivato dalla fase di crawling, su cui vengono eseguiti i test. Tali deduzioni derivano dal fatto che la distanza tra il vettore di \textit{trustrank} ricavato offline è il vettore di \textit{TrustRank} ricavato durante la l'esecuzione online, diminuisce con l'aumentare della dimensione del grafo temporaneo su cui viene calcolato \textit{TrustRank} online (\textit{test numero 1}). Le stesse considerazioni valgono per \textit{Anti-Trust Rank}. Inoltre i risultati dei test  indicano che fin dall'inizio del crawling le prestazioni degli algoritmi in modalità online sono abbastanza affidabili, infatti i valori delle Tau di Kendall applicate tra il vettore ottenuto in modalità online, nelle prime fasi del crawling, con il vettore ottenuto in modalità offline, sono abbastanza alti; in particolare nel \textit{Test numero 1} i valori delle Tau di Kendall,  nell'esame di \textit{TrustRank} ha come valore iniziale 0.7, e nell'esame di \textit{Anti-Trust Rank} ha come valore iniziale 0.75. Questo implica che la distanza tra i due vettori all'inizio del crawling è molto piccola. Quindi è facile dedurre che questi due algoritmi di spam detection possono essere usati online.

Confrontando i due algoritmi evince che \textit{Anti-Trust Rank} approssima fin dall'inizio del crawling meglio il comportamento offline e quindi è più indicato per essere usato durante la fase di crawling. Ma i due algoritmi invertono le loro prestazioni al termine del crawling, quando il grafo temporaneo è molto simile al grafo completo ottenuto offline.

Un modo di utilizzare questi algoritmi online sarebbe quello di basarsi sull'isolamento approssimato dell'insieme delle pagine buone. Dal momento che le pagine non spam difficilmente avranno link verso pagine spam, si protrebbero utilizzare gli algoritmi di \textit{TrustRank} e \textit{Anti-Trust Rank} in modalità online per identificare le pagine spam e quindi un volta identificati i nodi spam penalizzare questi e quelli che hanno dei link verso tali nodi; una soluzione estrema potrebbe essere di elimnare tutte le pagine appartenenti ad un nodo identificato come spam.

Quindi dalle conclusioni della fase di testing risulta vantaggioso utilizzare questi algoritmi in modalità online avendo fin dall'iniziio del crawling delle buone perfomance, inoltre una volta identificate le pagine spam molte altre pagine potrebbero essere non scaricate e quindi si eseguirebbero dei calcoli su grafi molto più piccoli e quindi abbasere il bisogno di risorser computazionali e di memorizzazione.

Sviluppo futuro di tale lavoro sarà la progettazione di un modulo di spam detection da inserire all'interno del  web crawler \textit{BUbiNG}, sviluppato dal Laboratorio di Algoritmica per il Web dell'Università degli Studi di Milano.


